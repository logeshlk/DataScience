{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "from datetime import date\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random Forest model on BOW features\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-12-6899561d9a10>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-6899561d9a10>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def data_preprocessing(data,path,method='training', cv_path):\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(path,method='training', cv_path):\n",
    "        df=pd.read_csv(path,encoding = \"ISO-8859-1\")\n",
    "        df['Buckets Items'].replace({\"Environment Issue\": \"Application Issue\",\"Timing Exception\": \"Application Issue\", \"Infrastrature Issue\": \"Data related Issue\",\n",
    "                     'Timing Issue' : 'Data related Issue', 'Latency related issue' : 'Application Issue' }, inplace=True)\n",
    "        df['Predecessor TestCase ID'].fillna(0, inplace=True)\n",
    "        df['Successor TestCase ID'].fillna(0, inplace=True)\n",
    "        df['Execution Time'] = pd.to_datetime(df['Execution Time for test step'])\n",
    "        df['exe_hour'] = df['Execution Time'].dt.hour\n",
    "        df['exe_min'] = df['Execution Time'].dt.minute\n",
    "        df['exe_sec'] = df['Execution Time'].dt.second\n",
    "        df.drop(columns = {'Execution Time for test step','Execution Time'}, inplace=True)\n",
    "        df['Exception Name'] =  df['Exception Name'].str.cat(df['RCA'],sep=\" \")\n",
    "        df['Exception Name'].fillna('', inplace=True)\n",
    "        df.drop(columns={'RCA','Test Run Id'}, inplace=True)\n",
    "        df['Exception Name'] = df['Exception Name'].astype('str')\n",
    "        df1 = cv_conversion(pre_process_corpus(df['Exception Name']),method=method,cv_path)\n",
    "        \n",
    "        #Data_Preprocessing        \n",
    "        #df1 = cv_conversion(pre_process_corpus(df[['Exception Name','RCA']]))\n",
    "        \n",
    "        if method == 'training':\n",
    "            df['Buckets Items'].replace({\"Environment Issue\": \"Application Issue\",\"Timing Exception\": \"Application Issue\", \"Infrastrature Issue\": \"Data related Issue\",\n",
    "                     'Timing Issue' : 'Data related Issue', 'Latency related issue' : 'Application Issue' }, inplace=True)\n",
    "            \n",
    "            \n",
    "            #df2 = cv_conversion(pre_process_corpus(df['RCA']))\n",
    "           # df3 = pd.concat([df1,df2], axis=1)\n",
    "            data = pd.concat([df1,df], axis=1)\n",
    "            \n",
    "            data['Buckets Items'] = data['Buckets Items'].replace({\"Application Issue\":0, \"Automation Issue\":1, \"Data related Issue\":2})\n",
    "        else :\n",
    "            data = pd.concat([df1,df], axis=1)\n",
    "        \n",
    "        data.drop(columns={'Exception Name'},inplace=True)\n",
    "        \n",
    "             \n",
    "        #Target Renaming\n",
    "        data = data.rename(columns={'Buckets Items' : 'FailureReason'})\n",
    "        \n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function defined to remove unicode data\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case and remove special characters\\whitespaces\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower()\n",
    "    doc = remove_accented_chars(doc)\n",
    "    #doc = contractions.fix(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    doc = doc.strip()\n",
    "    doc=re.sub('[0-9]', '', doc)\n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_conversion(df,method):\n",
    "    \n",
    "    if method == 'training':\n",
    "        #df['Exception Name'] =  df['Exception Name'].str.cat(df['RCA'],sep=\" \")\n",
    "        #df['Exception Name'].fillna('', inplace=True)\n",
    "        #print (combined_2)\n",
    "        cv = CountVectorizer(ngram_range=(1, 1), min_df=0.001, max_df=0.99)\n",
    "        cv_exception_matrix = cv.fit_transform(df)\n",
    "        pickle.dump(cv, open(path, 'wb'))\n",
    "    else:\n",
    "        cv = pickle.load(open(path,'rb'))\n",
    "        cv_exception_matrix = cv.transform(df)\n",
    "        \n",
    "    \n",
    "    cv_exception_matrix=cv_exception_matrix.toarray()\n",
    "    # get all unique words in the corpus\n",
    "    vocab = cv.get_feature_names()\n",
    "    # show document feature vectors\n",
    "    data2=pd.DataFrame(cv_exception_matrix, columns=vocab)\n",
    "        \n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(data, path):\n",
    "    x = data.loc[:, data.columns != 'FailureReason']\n",
    "    y = data['FailureReason']\n",
    "    model = BaggingClassifier(n_estimators=135)\n",
    "    \n",
    "    model = model.fit(x,y)\n",
    "        \n",
    "#     path = \"\\\".join([path, filename])\n",
    "    pickle.dump(model, open(path, 'wb'))\n",
    "    \n",
    "    return 'Model has been trained successfully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(data,path):\n",
    "    x = data.loc[:, data.columns != 'FailureReason']    \n",
    "    # load the model from disk\n",
    "    loaded_model = pickle.load(open(path,'rb'))\n",
    "    \n",
    "    result = loaded_model.predict(x)\n",
    "    pred_result = pd.DataFrame(result, columns=['FailureReason'])\n",
    "    pred_result['FailureReason'] = pred_result['FailureReason'].replace({0:'Application Issue',1: 'Automation Issue', 2: 'Data related Issue'})\n",
    "    #data = pd.concat([x,pred_result], axis=1)\n",
    "#     filename = 'output_{}.csv'.format(date.today())\n",
    "#     data.to_csv(filename,index=False)\n",
    "    \n",
    "    return pred_result  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(data,path='UC_Sample Data.csv'):\n",
    "    ip = pd.read_csv(path,encoding = \"ISO-8859-1\")\n",
    "    ip.drop(columns = 'Buckets Items', inplace=True)\n",
    "    ip = ip.rename(columns={'Test Case ID' : 'TestCaseID',\n",
    "                            'Failed Steps' : 'FailedSteps',\n",
    "                            'Exception Name': 'ExceptionName'})\n",
    "    op = pd.concat([ip,data], axis=1)\n",
    "    filename = 'output_{}.csv'.format(date.today())\n",
    "    op.to_csv(filename,index=False)\n",
    "    df = pd.DataFrame(op, columns = ['TestCaseID','FailedSteps','ExceptionName','FailureReason'])\n",
    "    output  = df.to_json(r'UC_27.json',orient='records')\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\45448\\\\Documents\\\\ATOP\\\\UC_27'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data, method,path = 'C:\\\\Users\\\\45448\\\\Documents\\\\ATOP\\\\UC_27\\\\uc27_train',\n",
    "         cvpath='C:\\\\Users\\\\45448\\\\Documents\\\\ATOP\\\\UC_27\\\\'):\n",
    "    if method == 'training' :\n",
    "        df_train = data_preprocessing(path = 'UC_Sample Data.csv',method=method)\n",
    "        status = model_training(df_train, path)\n",
    "    \n",
    "    else :\n",
    "        df_pred = data_preprocessing(path = 'UC_Sample Data.csv',method=method)\n",
    "        df_pred = model_predict(df_pred,'C:\\\\Users\\\\45448\\\\Documents\\\\ATOP\\\\UC_27\\\\uc27')\n",
    "        print ('Model has predicted the Failure Reason')\n",
    "        op = output(df_pred)\n",
    "    return op if status is None else status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_preprocessing()\n",
    "df\n",
    "#model_training(df, 'C:\\\\Users\\\\45448\\\\Documents\\\\ATOP\\\\UC_27\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6389e735882d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'UC_Sample Data.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = data_preprocessing(path = 'UC_Sample Data.csv',method='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acceptinsecurecerts      0\n",
       "acceptsslcerts           0\n",
       "access                   0\n",
       "accessible               0\n",
       "accessing                0\n",
       "                        ..\n",
       "Successor TestCase ID    0\n",
       "FailureReason            0\n",
       "exe_hour                 0\n",
       "exe_min                  0\n",
       "exe_sec                  0\n",
       "Length: 400, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been successfully trained\n"
     ]
    }
   ],
   "source": [
    "model_training(df_train, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-f5db32d1c500>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "os.path.dirname(os.path.realpath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 4222.54it/s]\n"
     ]
    }
   ],
   "source": [
    "df_pred = data_preprocessing(path = 'UC_Sample Data.csv',method='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = model_predict(df_pred,'C:\\\\Users\\\\45448\\\\Documents\\\\ATOP\\\\UC_27\\\\uc27_2020-08-17' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "output(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
